import requests
from bs4 import BeautifulSoup
import csv

# Function to scrape product data from Jumia
def scrape_jumia_products(url, max_pages=50):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    }
    product_data = []

    for page in range(1, max_pages + 1):
        print(f"Scraping page {page}...")
        response = requests.get(f"{url}?page={page}", headers=headers)

        if response.status_code != 200:
            print(f"Failed to fetch page {page}")
            continue

        soup = BeautifulSoup(response.content, "html.parser")
        products = soup.find_all("article", class_="prd _fb col c-prd")

        for product in products:
            try:
                # Basic product information
                name = product.find("h3", class_="name").text.strip()
                image_tag = product.find("img")
                image_url = image_tag.get("data-src") or image_tag.get("src")
                sku = product.get("sku-config")
                product_id = product.get("product-id")
                price = product.find("div", class_="prc").text.strip() if product.find("div", class_="prc") else "N/A"
                original_price = product.find("div", class_="old").text.strip() if product.find("div", class_="old") else "N/A"
                discount = product.find("div", class_="bdg _dsct").text.strip() if product.find("div", class_="bdg _dsct") else "N/A"

                # Ratings
                stars = product.find("div", class_="stars _s").get("style") if product.find("div", class_="stars _s") else "N/A"
                ratings = product.find("div", class_="rev").text.strip() if product.find("div", class_="rev") else "N/A"

                # Category Information (assuming category path exists)
                full_cat_path = product.get("category")  # Full category path
                primary_category = full_cat_path.split(">")[0] if full_cat_path else "N/A"
                last_subcategory = full_cat_path.split(">")[-1] if full_cat_path else "N/A"

                # Other details
                seller_name = product.find("div", class_="seller-name").text.strip() if product.find("div", class_="seller-name") else "Unknown"
                sku_config = product.get("sku-config", "N/A")
                mpg = product.get("mpg", "N/A")
                brand = product.get("brand", "N/A")
                shipment_type = product.get("shipment-type", "N/A")
                badge = product.find("div", class_="tag").text.strip() if product.find("div", class_="tag") else "N/A"
                tag_collection = [tag.text.strip() for tag in product.find_all("div", class_="tag-collection")] or "N/A"
                store_path = product.get("store-path", "N/A")

                # Append collected data
                product_data.append({
                    "Product ID": product_id,
                    "Seller Name": seller_name,
                    "Product Name": name,
                    "SKU Config": sku_config,
                    "MPG": mpg,
                    "Brand": brand,
                    "Image URL": image_url,
                    "Primary Category": primary_category,
                    "Last Subcategory": last_subcategory,
                    "Full Category Path": full_cat_path,
                    "Shipment Type": shipment_type,
                    "Price": price,
                    "Tag": badge,
                    "Badge": badge,
                    "Tag Collection": tag_collection,
                    "Store Path": store_path,
                    "Original Price": original_price,
                    "Discount (%)": discount,
                    "Stars": stars,
                    "Ratings": ratings
                })
            except AttributeError:
                continue

    return product_data

# Save the scraped data to a CSV file
def save_to_csv(data, filename="jumia_products_detailed.csv"):
    keys = data[0].keys()
    with open(filename, "w", newline="", encoding="utf-8") as output_file:
        dict_writer = csv.DictWriter(output_file, fieldnames=keys)
        dict_writer.writeheader()
        dict_writer.writerows(data)

# URL of the Jumia category page to scrape
category_url = "https://www.jumia.com.ng/cell-phones/"  # Replace with the actual category URL

# Scrape the data and save to CSV
data = scrape_jumia_products(category_url, max_pages=50)
if data:
    save_to_csv(data)
    print("Data saved to jumia_products_detailed.csv")
else:
    print("No data found.")
